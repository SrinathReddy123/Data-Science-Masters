{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9254b92e-e365-4f05-8163-ff7fd6236205",
   "metadata": {},
   "source": [
    "Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf488d8-d6df-4064-8b7b-97ee3f267292",
   "metadata": {},
   "source": [
    "Decision tree classifier is a popular and widely used algorithm in machine learning for both classification and regression tasks. It is a tree-structured model that makes predictions by recursively partitioning the input space into smaller regions based on a set of splitting rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20b3f4-c939-4e12-9d1b-1d6a5d7ce52f",
   "metadata": {},
   "source": [
    "The algorithm works by creating a tree of decisions, where each node in the tree represents a decision based on one of the input features. The root node represents the entire dataset, and each subsequent node represents a partition of the data based on a feature value. The leaf nodes represent the final classification or regression prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5de543-63a7-4dae-b1f1-1e5289e7a8c7",
   "metadata": {},
   "source": [
    "The process of building a decision tree classifier involves the following steps:\n",
    "\n",
    "Feature selection: The algorithm selects the best feature to split the data into subsets based on a particular criterion such as entropy or Gini impurity.\n",
    "\n",
    "Splitting the data: Once a feature is selected, the algorithm splits the dataset into two or more subsets based on the value of the selected feature.\n",
    "\n",
    "Recursive splitting: The above two steps are recursively repeated on each subset until a stopping criterion is met, such as the maximum depth of the tree, minimum number of samples in a leaf node, or when further splitting does not improve the classification accuracy.\n",
    "\n",
    "Assigning class labels: After building the decision tree, it assigns class labels to each leaf node by either majority voting (for classification) or averaging (for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97011e76-814b-4ac2-81ea-0b38f9a42d5b",
   "metadata": {},
   "source": [
    "To make a prediction on a new input, the decision tree algorithm starts at the root node and moves down the tree based on the values of the input features. At each node, it evaluates the value of the corresponding feature and moves to the child node that matches the feature value. This process is repeated until the algorithm reaches a leaf node, where it returns the corresponding class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6a51e-926c-4bd7-bb47-04020774c5c6",
   "metadata": {},
   "source": [
    "One advantage of decision tree classifiers is that they are easy to interpret and visualize. However, they are prone to overfitting, especially when the trees are deep and complex. Therefore, it is important to carefully tune the hyperparameters of the algorithm to avoid overfitting and improve the generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d113fb0-04c2-4e2d-9863-e112f1f34b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5ec05-0418-4042-b615-33655ff16339",
   "metadata": {},
   "source": [
    "Decision tree classification is a machine learning algorithm that uses a tree-like model to make predictions based on input features. The tree structure represents a hierarchy of decisions based on the features, where each node in the tree corresponds to a test on one of the features and each branch represents a possible outcome of the test. The leaves of the tree correspond to the predicted class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb7ef0-10f4-499b-86e4-13dc48e4d076",
   "metadata": {},
   "source": [
    "The mathematical intuition behind decision tree classification involves two main concepts: information gain and entropy.\n",
    "\n",
    "Information gain is a measure of the reduction in entropy achieved by splitting the data on a particular feature. Entropy is a measure of the randomness or uncertainty in the data. The goal of the algorithm is to maximize the information gain at each step to achieve the most significant reduction in entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad474c2-a430-46f1-9154-31065649c7bc",
   "metadata": {},
   "source": [
    "Here is a step-by-step explanation of the mathematical intuition behind decision tree classification:\n",
    "\n",
    "Define entropy: Entropy is defined as the measure of the amount of uncertainty or randomness in the data. Mathematically, the entropy of a dataset S is given by:\n",
    "\n",
    "E(S) = -p_1 log2 p_1 - p_2 log2 p_2 - ... - p_k log2 p_k\n",
    "\n",
    "where p_1, p_2, ..., p_k are the proportions of each class label in the dataset S.\n",
    "\n",
    "Calculate the entropy of the original dataset: Calculate the entropy of the original dataset before any splits are made. This entropy value will be used as a reference to measure the information gain achieved by each split.\n",
    "\n",
    "Calculate the information gain: For each feature, calculate the information gain achieved by splitting the dataset on that feature. Information gain is defined as the difference between the entropy of the original dataset and the weighted sum of entropies of the resulting subsets after the split. Mathematically, the information gain IG(S, A) of a feature A with respect to a dataset S is given by:\n",
    "\n",
    "IG(S, A) = E(S) - âˆ‘_v (|S_v| / |S|) * E(S_v)\n",
    "\n",
    "where |S_v| is the number of samples in the subset S_v that have the value v of feature A, and |S| is the total number of samples in the dataset S.\n",
    "\n",
    "The information gain value for each feature is used to determine which feature to split on at each node of the decision tree.\n",
    "\n",
    "Repeat the process recursively: The above steps are repeated recursively for each subset of data resulting from each split until all leaf nodes are pure (i.e., contain only samples of the same class label) or the tree reaches a maximum depth or a minimum number of samples in a leaf node.\n",
    "\n",
    "Assign class labels: After building the decision tree, the class label of a new sample is predicted by traversing the tree from the root node to a leaf node based on the values of the features in the sample. The predicted class label is the majority class label of the samples in the leaf node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082531c-e94a-46b4-add8-2876c9b57c61",
   "metadata": {},
   "source": [
    "In summary, decision tree classification involves calculating the entropy and information gain at each node to recursively split the data based on the most informative feature. This process results in a tree-like model that can be used to predict the class label of new samples based on their feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f11552-08da-4d36-83c1-84c4ed2b1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe1a3f-f000-4761-a48d-feca1f5a2fbb",
   "metadata": {},
   "source": [
    "A decision tree classifier can be used to solve a binary classification problem by recursively partitioning the input space into smaller regions based on a set of splitting rules until a stopping criterion is met. The leaf nodes of the tree represent the final binary classification prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6e75b-9b58-4afd-9119-8e5169ff37f2",
   "metadata": {},
   "source": [
    "Here is a step-by-step explanation of how a decision tree classifier can be used to solve a binary classification problem:\n",
    "\n",
    "Prepare the data: The input data should be preprocessed and divided into training and test sets. The training set is used to build the decision tree classifier, while the test set is used to evaluate the performance of the model.\n",
    "\n",
    "Define the stopping criterion: The stopping criterion is used to terminate the tree-building process. It could be a maximum tree depth, a minimum number of samples required to split a node, or a minimum improvement in the classification performance achieved by a split.\n",
    "\n",
    "Choose the splitting criterion: The splitting criterion is used to determine which feature to split on at each node of the tree. For binary classification, the most commonly used splitting criteria are Gini impurity and entropy. Gini impurity measures the probability of misclassifying a sample from a randomly chosen class, while entropy measures the level of disorder or uncertainty in the data.\n",
    "\n",
    "Build the decision tree: The decision tree classifier is built recursively by selecting the best feature to split the data based on the splitting criterion. The feature with the highest information gain is chosen to split the data, which maximizes the reduction in the impurity or entropy of the resulting subsets. The process is repeated until the stopping criterion is met.\n",
    "\n",
    "Predict the class labels: To predict the class label of a new sample, the decision tree classifier traverses the tree from the root node to a leaf node based on the values of the input features. At each node, the classifier evaluates the value of the corresponding feature and moves to the child node that matches the feature value. This process is repeated until the classifier reaches a leaf node, where it returns the corresponding binary class label.\n",
    "\n",
    "Evaluate the performance: The performance of the decision tree classifier is evaluated on the test set by computing metrics such as accuracy, precision, recall, and F1 score. These metrics provide a measure of the classifier's ability to correctly classify new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073d83e-5e95-4983-951f-d97d98bfc643",
   "metadata": {},
   "source": [
    "In summary, a decision tree classifier can be used to solve a binary classification problem by recursively partitioning the input space into smaller regions based on a set of splitting rules until a stopping criterion is met. The leaf nodes of the tree represent the final binary classification prediction, and the performance of the classifier is evaluated on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb16a20-d3d9-4285-aaa5-cec7d3d5da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd06c0-c005-489b-afff-0284b2e64d8b",
   "metadata": {},
   "source": [
    "The geometric intuition behind decision tree classification involves dividing the input space into smaller regions based on a set of splitting rules, where each region corresponds to a leaf node of the decision tree. The splitting rules are based on the values of the input features, and the goal is to maximize the separation between the different binary classes in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e994e3-c110-4965-9980-af4409cd5e22",
   "metadata": {},
   "source": [
    "\n",
    "To understand this concept, consider a simple binary classification problem with two input features, x1 and x2. The input space can be represented as a two-dimensional plane, where each point corresponds to a pair of values (x1, x2). The binary classes are represented by different colors, such as blue and red, as shown in the figure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7677a80-7dc5-474a-a94c-52970f99c4b5",
   "metadata": {},
   "source": [
    "Decision Tree Classification Geometric Intuition\n",
    "\n",
    "The goal of the decision tree classifier is to divide the input space into smaller regions, where each region corresponds to a different binary class prediction. The splitting rules are based on the values of the input features, and each split creates a new boundary in the input space. For example, the first split could be based on the value of x1, creating two regions separated by a vertical line. The second split could be based on the value of x2, creating four regions separated by two perpendicular lines.\n",
    "\n",
    "The decision tree classifier continues to recursively partition the input space based on the values of the input features until a stopping criterion is met. The final regions correspond to the leaf nodes of the decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816278b-12ad-4db0-8e24-f961adbb4380",
   "metadata": {},
   "source": [
    "To make a prediction for a new sample, the decision tree classifier traverses the tree from the root node to a leaf node based on the values of the input features. At each node, the classifier evaluates the value of the corresponding feature and moves to the child node that matches the feature value. This process is repeated until the classifier reaches a leaf node, where it returns the corresponding binary class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4191d-cc41-4fa9-9561-484ad9b545ba",
   "metadata": {},
   "source": [
    "In summary, the geometric intuition behind decision tree classification involves dividing the input space into smaller regions based on a set of splitting rules, where each region corresponds to a leaf node of the decision tree. The goal is to maximize the separation between the different binary classes in the input space, and the final regions correspond to the leaf nodes of the decision tree, where each leaf node represents a different binary class prediction. To make a prediction for a new sample, the decision tree classifier traverses the tree from the root node to a leaf node based on the values of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546fb7d-3d72-4178-88ea-609649d57593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5840f2d-7622-4b45-b1d3-56bd1c391434",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e99c3d-62ce-4fd1-b447-909bbe37467e",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to evaluate the performance of a classification model by comparing the predicted class labels with the actual class labels. The table consists of four cells, where each cell represents a combination of predicted and actual class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61cc472-a663-4c8c-9f33-5a62a874b33a",
   "metadata": {},
   "source": [
    "The four cells of the confusion matrix are:\n",
    "\n",
    "True Positive (TP): The number of samples that are correctly predicted as positive (i.e., the model correctly identifies the positive samples).\n",
    "\n",
    "False Positive (FP): The number of samples that are incorrectly predicted as positive (i.e., the model incorrectly identifies the negative samples as positive).\n",
    "\n",
    "False Negative (FN): The number of samples that are incorrectly predicted as negative (i.e., the model incorrectly identifies the positive samples as negative).\n",
    "\n",
    "True Negative (TN): The number of samples that are correctly predicted as negative (i.e., the model correctly identifies the negative samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb044fe-847b-439b-81a4-9cbe76c1d166",
   "metadata": {},
   "source": [
    "The confusion matrix can be used to calculate several evaluation metrics, such as accuracy, precision, recall, and F1 score. These metrics provide a measure of the classification model's performance in terms of how well it correctly identifies the positive and negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e416cd-977a-44ae-bf1a-571e3ffaef26",
   "metadata": {},
   "source": [
    "Accuracy: The proportion of correctly classified samples out of the total number of samples. It is calculated as (TP+TN)/(TP+FP+FN+TN).\n",
    "\n",
    "Precision: The proportion of correctly identified positive samples out of the total number of predicted positive samples. It is calculated as TP/(TP+FP).\n",
    "\n",
    "Recall (Sensitivity): The proportion of correctly identified positive samples out of the total number of actual positive samples. It is calculated as TP/(TP+FN).\n",
    "\n",
    "F1 score: The harmonic mean of precision and recall. It is calculated as 2*(precision * recall)/(precision+recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f73adf-66e0-4d64-b7c7-88777fba90f1",
   "metadata": {},
   "source": [
    "In summary, the confusion matrix is a table that compares the predicted and actual class labels of a classification model, and it is used to calculate several evaluation metrics that provide a measure of the model's performance. The four cells of the confusion matrix represent the number of true positives, false positives, false negatives, and true negatives, which can be used to calculate metrics such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596408d5-84ef-42c4-99bd-a4219a807330",
   "metadata": {},
   "source": [
    "Answer 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931475eb-2cbf-468e-a499-e6c78b68cfc2",
   "metadata": {},
   "source": [
    "Let's consider an example of a binary classification problem where we want to predict whether a patient has a disease or not. We have a dataset of 100 samples, where 60 samples are disease-free (negative class) and 40 samples have the disease (positive class). We train a binary classification model on this dataset and obtain the following confusion matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff40b15-25de-46ec-b116-5aa71f2e30b5",
   "metadata": {},
   "source": [
    "\tPredicted Negative\tPredicted Positive\n",
    "Actual Negative\t50 (TN)\t10 (FP)\n",
    "Actual Positive\t5 (FN)\t35 (TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded3d47-9f9d-40c6-83a1-c0fcf3137378",
   "metadata": {},
   "source": [
    "From this confusion matrix, we can calculate various evaluation metrics:\n",
    "\n",
    "Accuracy: The accuracy of the model is (50+35)/(50+10+5+35) = 0.85 or 85%.\n",
    "\n",
    "Precision: The precision of the model is 35/(10+35) = 0.78 or 78%.\n",
    "\n",
    "Recall: The recall (sensitivity) of the model is 35/(5+35) = 0.88 or 88%.\n",
    "\n",
    "F1 Score: The F1 score of the model is 2*(0.78 * 0.88)/(0.78+0.88) = 0.83 or 83%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bffb4a-2878-4c96-b5b3-6fc60a5b718f",
   "metadata": {},
   "source": [
    "Here, precision represents the proportion of correctly identified positive samples out of the total number of predicted positive samples, which is 35 out of 45 (TP+FP). Recall represents the proportion of correctly identified positive samples out of the total number of actual positive samples, which is 35 out of 40 (TP+FN). The F1 score is a harmonic mean of precision and recall, which takes into account both precision and recall to provide a single metric for the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410914cc-4301-4c0f-a607-cd562fc9e01e",
   "metadata": {},
   "source": [
    "In this example, the model has a high accuracy, but a lower precision and recall, indicating that it is better at identifying true negatives (disease-free patients) than true positives (patients with the disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67d77a-5993-42f6-9eca-0639ee08b1ae",
   "metadata": {},
   "source": [
    "Answer 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cda336-1e53-4ba3-b932-f390c8528617",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric is crucial for assessing the performance of a classification model because it provides a measure of how well the model is predicting the correct class labels. However, the choice of evaluation metric depends on the problem and the objective of the model. For instance, in some cases, it may be more important to minimize false positives, while in other cases, it may be more important to minimize false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff73cbd-d76b-488b-8a17-2f2fea68cb72",
   "metadata": {},
   "source": [
    "For instance, in a spam detection problem, false positives (legitimate emails marked as spam) are less severe than false negatives (spam emails not detected). In contrast, in a medical diagnosis problem, false negatives (disease not detected in a sick patient) are more severe than false positives (healthy patient diagnosed with a disease). Hence, the choice of evaluation metric should be aligned with the problem and the objective of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef61e47-1585-4bed-9164-17a9c954f953",
   "metadata": {},
   "source": [
    "Here are some commonly used evaluation metrics for classification problems:\n",
    "\n",
    "Accuracy: The proportion of correctly classified samples out of the total number of samples. It is a good metric when the number of samples in each class is balanced.\n",
    "\n",
    "Precision: The proportion of correctly identified positive samples out of the total number of predicted positive samples. It is a good metric when minimizing false positives is important.\n",
    "\n",
    "Recall (Sensitivity): The proportion of correctly identified positive samples out of the total number of actual positive samples. It is a good metric when minimizing false negatives is important.\n",
    "\n",
    "F1 score: The harmonic mean of precision and recall. It is a good metric when both precision and recall are important.\n",
    "\n",
    "Specificity: The proportion of correctly identified negative samples out of the total number of actual negative samples. It is a good metric when minimizing false positives is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21144aa-882f-4f53-a70b-38a2db3ffb77",
   "metadata": {},
   "source": [
    "In order to choose an appropriate evaluation metric for a classification problem, it is important to consider the problem domain, the costs associated with false positives and false negatives, and the goals of the model. Once an appropriate metric is selected, the model can be trained and evaluated using that metric to optimize the model's performance for the specific problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "340a5e32-2779-43d5-b4a0-445418f8687c",
   "metadata": {},
   "source": [
    "Answer 8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c280149-6e58-4da4-b151-010f664d0a61",
   "metadata": {},
   "source": [
    "An example of a classification problem where precision is the most important metric is fraud detection in financial transactions. In this problem, the objective is to detect fraudulent transactions and prevent financial losses. In this case, false positives (legitimate transactions marked as fraudulent) are less severe than false negatives (fraudulent transactions not detected), as the former can be easily reversed or resolved, while the latter can lead to significant financial losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b080c-1887-4c01-a72d-890aa847e5a2",
   "metadata": {},
   "source": [
    "Thus, precision is the most important metric in this problem because it measures the proportion of correctly identified fraudulent transactions out of the total number of predicted fraudulent transactions. A high precision indicates that the model is identifying mostly fraudulent transactions and minimizing the number of false positives. This, in turn, minimizes the number of legitimate transactions that are wrongly flagged as fraudulent, reducing the risk of financial losses and increasing the overall effectiveness of the fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3313f-f6a1-407f-94e9-08e94b1461f7",
   "metadata": {},
   "source": [
    "Recall (sensitivity) is also important in this problem because it measures the proportion of correctly identified fraudulent transactions out of the total number of actual fraudulent transactions. A high recall indicates that the model is identifying most of the fraudulent transactions, reducing the number of false negatives and increasing the overall effectiveness of the fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132df4c9-ced2-4a72-931a-86953011f976",
   "metadata": {},
   "source": [
    "However, in this problem, precision is more important than recall because the cost of a false positive is relatively low compared to the cost of a false negative. A false positive can be easily resolved by reviewing the transaction and reversing the decision, whereas a false negative can lead to significant financial losses. Hence, it is crucial to prioritize precision over recall in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6753d9f-0c67-4132-9751-d8e66293a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 9:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70f1cd-2bce-4189-815d-beb388cff101",
   "metadata": {},
   "source": [
    "An example of a classification problem where recall is the most important metric is cancer diagnosis. In this problem, the objective is to detect cancerous tumors in patients and provide timely treatment to improve their chances of survival. In this case, false negatives (cancerous tumors not detected) are more severe than false positives (non-cancerous tumors wrongly identified as cancerous), as the former can lead to delayed or missed treatment, reducing the patient's chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7515c4-802d-4512-8d17-be7e60178efb",
   "metadata": {},
   "source": [
    "Thus, recall is the most important metric in this problem because it measures the proportion of correctly identified cancerous tumors out of the total number of actual cancerous tumors. A high recall indicates that the model is identifying most of the cancerous tumors, reducing the number of false negatives and increasing the chances of timely treatment and improved survival rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c25ec-3f00-4bad-8a11-a88049065f02",
   "metadata": {},
   "source": [
    "Precision is also important in this problem because it measures the proportion of correctly identified cancerous tumors out of the total number of predicted cancerous tumors. A high precision indicates that the model is identifying mostly cancerous tumors and minimizing the number of false positives. This, in turn, reduces the number of unnecessary biopsies or treatments for non-cancerous tumors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21641fa2-7bfe-4f13-9fa9-c7ce058efdff",
   "metadata": {},
   "source": [
    "However, in this problem, recall is more important than precision because the cost of a false negative is relatively high compared to the cost of a false positive. A false negative can lead to delayed or missed treatment, reducing the patient's chances of survival. Hence, it is crucial to prioritize recall over precision in this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

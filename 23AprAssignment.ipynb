{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b11268-f2c0-4338-a008-ec69e3f4cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ca8fe-1e53-4f3d-b7d7-ae6db7edd68a",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to the problems that arise when working with high-dimensional data, where the number of features or dimensions is much larger than the number of samples. As the number of dimensions increases, the volume of the feature space grows exponentially, making it increasingly difficult to accurately and efficiently model and analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff23fbb-758f-482c-b3ad-e7e776d569ba",
   "metadata": {},
   "source": [
    "There are several problems associated with the curse of dimensionality, including:\n",
    "\n",
    "1.Increased computational complexity: As the number of dimensions increases, the computational complexity of modeling and analyzing the data also increases, making it difficult to scale up to large datasets.\n",
    "\n",
    "2.Increased sparsity: In high-dimensional spaces, the number of samples required to obtain a representative sample of the feature space increases exponentially, leading to sparse datasets where many regions of the feature space are empty.\n",
    "\n",
    "3.Overfitting: High-dimensional data is more prone to overfitting, where models become too complex and fit the noise in the data rather than the underlying patterns.\n",
    "\n",
    "4.Difficulty in visualizing the data: As the number of dimensions increases, it becomes increasingly difficult to visualize the data and gain insights into its structure and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c9d9f-f1f7-4f55-9fc9-485057c47b96",
   "metadata": {},
   "source": [
    "To address the problems associated with the curse of dimensionality, dimensionality reduction techniques are often used in machine learning. \n",
    "\n",
    "These techniques aim to reduce the number of dimensions in the data while preserving as much of the relevant information as possible. By reducing the number of dimensions, these techniques can help to reduce computational complexity, increase data density, and reduce the risk of overfitting. \n",
    "\n",
    "Some commonly used dimensionality reduction techniques include principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and manifold learning techniques such as Isomap and locally linear embedding (LLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4959d-603b-4c2b-9da7-a979e5c82623",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0d7ab-9b10-4e99-bf94-ce15ccc6ced0",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the phenomenon where the performance of machine learning algorithms degrades as the number of features or dimensions in the dataset increases. It has several impacts on algorithm performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395edf8c-caec-4e44-b72e-0edde5d6ccb3",
   "metadata": {},
   "source": [
    "1. Increased Sparsity: As the number of dimensions increases, the available data becomes sparser in the high-dimensional space. This sparsity can lead to difficulties in accurately estimating statistical properties, making it challenging for algorithms to find meaningful patterns or relationships.\n",
    "\n",
    "2. Increased Computational Complexity: With more dimensions, the computational complexity of algorithms typically increases exponentially. Many machine learning algorithms rely on distance calculations or similarity measures, which become more computationally expensive as the dimensionality grows. This can result in longer training and prediction times\n",
    "\n",
    "3. Increased Sample Size Requirement: As the dimensionality increases, the number of samples required to maintain statistical significance and generalize well also increases. Insufficient data points relative to the number of dimensions can lead to overfitting, where models perform well on training data but fail to generalize to unseen data.\n",
    "\n",
    "4. Curse of Dimensionality in Distance Metrics: Distance-based algorithms, such as k-nearest neighbors, can suffer from the curse of dimensionality. In high-dimensional spaces, the distances between points become less informative due to the increased spread of data points. Consequently, the effectiveness of distance-based algorithms decreases as the dimensionality grows.\n",
    "\n",
    "5. Feature Irrelevance and Redundancy: In high-dimensional datasets, many features may be irrelevant or redundant for the target task. This can introduce noise and hinder the learning process, making it difficult for algorithms to extract meaningful and discriminative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5375c1a-d824-48ef-b71f-61267ae3ac51",
   "metadata": {},
   "source": [
    "To mitigate the curse of dimensionality, various techniques can be employed, such as dimensionality reduction (e.g., PCA, t-SNE), feature selection, or feature engineering. These methods aim to reduce the dimensionality of the data while preserving relevant information and improving algorithm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697c75d-194e-4c55-94fe-89813efbb5fd",
   "metadata": {},
   "source": [
    "It is crucial to consider the curse of dimensionality when working with high-dimensional datasets, as it can significantly impact the performance, accuracy, and efficiency of machine learning algorithms. \n",
    "\n",
    "Careful feature selection, dimensionality reduction, and appropriate algorithm choice are essential to overcome the challenges posed by high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e7f84-6545-4f3a-8401-cf9f7decf950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d68d8-281b-455a-9781-12449117ecb5",
   "metadata": {},
   "source": [
    "The curse of dimensionality has several consequences in machine learning, which can significantly impact model performance. Here are some key consequences:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18b41b-5458-46f3-9033-0100c5bc7fe2",
   "metadata": {},
   "source": [
    "1. Increased Data Sparsity: As the number of dimensions increases, the available data becomes sparser. In high-dimensional spaces, the data points are more spread out, resulting in fewer data points in close proximity to each other. This sparsity makes it challenging for machine learning algorithms to accurately estimate statistical properties and find meaningful patterns in the data.\n",
    "\n",
    "2. Increased Computational Complexity: The curse of dimensionality leads to increased computational complexity. Many machine learning algorithms rely on distance calculations, similarity measures, or optimization procedures, which become more computationally expensive as the dimensionality grows. As a result, training and inference times can become prohibitively long as the number of dimensions increases.\n",
    "\n",
    "3. Overfitting and Generalization Challenges: With a high-dimensional dataset, models become more prone to overfitting. Overfitting occurs when a model learns to fit noise or irrelevant features in the data, resulting in poor generalization to unseen data. The increased dimensionality requires a larger sample size to maintain statistical significance and to effectively learn patterns. Insufficient data points relative to the number of dimensions can lead to overfitting and poor generalization performance.\n",
    "\n",
    "4. Increased Feature Irrelevance and Redundancy: In high-dimensional datasets, many features may be irrelevant or redundant for the target task. This can introduce noise and hinder the learning process, making it difficult for algorithms to extract meaningful and discriminative features. The presence of irrelevant or redundant features can increase the complexity of the model and degrade its performance.\n",
    "\n",
    "5. Difficulties in Visualization and Interpretability: Visualizing and interpreting high-dimensional data becomes challenging due to limitations in human perception and the complexity of visualizing data beyond three dimensions. Understanding the relationships and structures within the data becomes more difficult as the number of dimensions increases, making it harder to gain insights from the data and interpret the model's behavior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8b31f-83e5-4e16-9898-29fd6e104e9e",
   "metadata": {},
   "source": [
    "To mitigate the consequences of the curse of dimensionality, various techniques can be employed. These include dimensionality reduction techniques (e.g., PCA, t-SNE), feature selection methods, and regularization techniques to reduce model complexity and improve generalization. \n",
    "\n",
    "It's important to carefully preprocess and analyze high-dimensional data to mitigate the negative impacts of the curse of dimensionality and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad8ca4-92bf-4918-b979-bc5e4c56d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdeca31-8cff-451a-adca-9adf81c0f37b",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant features from a larger set of available features in a dataset. It aims to identify and retain the most informative and discriminative features while discarding irrelevant or redundant ones. \n",
    "\n",
    "Feature selection can help with dimensionality reduction by reducing the number of features considered by a machine learning model, thereby mitigating the curse of dimensionality and improving model performance. Here's how feature selection works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956b5c4-0a35-4faf-a4c6-ea2879dea5ad",
   "metadata": {},
   "source": [
    "1. Importance Ranking: Feature selection methods typically start by evaluating the importance or relevance of each feature in relation to the target variable. Various techniques can be used to estimate feature importance, such as statistical tests, correlation analysis, or algorithm-specific feature importance measures.\n",
    "\n",
    "2. Selection Criteria: Based on the importance rankings, a selection criterion is defined to choose the most relevant features. Common criteria include selecting the top-k features with the highest scores, selecting a fixed percentage of the most important features, or using a threshold value to select features above a certain level of importance.\n",
    "\n",
    "3. Feature Subset Selection: Once the selection criterion is determined, the feature selection process narrows down the feature set to the selected subset. The remaining features form the reduced feature space that is used for subsequent modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfaab7c-ab58-4d56-8fdc-39e2de72deac",
   "metadata": {},
   "source": [
    "Feature selection can offer several benefits:\n",
    "\n",
    "1. Improved Model Performance: By selecting the most informative features, feature selection can enhance the performance of machine learning models. Removing irrelevant or redundant features reduces noise, focuses the model on the most relevant information, and facilitates better generalization.\n",
    "\n",
    "2. Reduced Overfitting: Feature selection reduces the risk of overfitting, especially in high-dimensional datasets. By eliminating irrelevant features that may introduce noise or bias, the model becomes less prone to fitting spurious patterns and improves its ability to generalize to unseen data.\n",
    "\n",
    "3. Computational Efficiency: With a reduced feature set, the computational complexity of training and inference decreases. The model requires fewer resources and less time to process and make predictions, improving efficiency.\n",
    "\n",
    "4. Interpretability and Insights: Feature selection can improve the interpretability of machine learning models by focusing on a smaller subset of features. It enables easier understanding of the relationships between the selected features and the target variable, providing insights into the underlying data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da03d0-796c-4ce6-bfeb-e734b3384a33",
   "metadata": {},
   "source": [
    "Feature selection methods can be classified into three broad categories: filter methods, wrapper methods, and embedded methods. Filter methods assess feature relevance independently of any specific machine learning algorithm. \n",
    "\n",
    "Wrapper methods evaluate feature subsets by training and evaluating the model on different subsets. Embedded methods incorporate feature selection as part of the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f6421-f751-4c76-9fcd-0019c3846818",
   "metadata": {},
   "source": [
    "Overall, feature selection plays a crucial role in dimensionality reduction by identifying the most relevant features, improving model performance, reducing overfitting, and enhancing interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b8c4a-e79b-4e21-b3aa-2400cfa39784",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9cc72-eaff-47d6-86fc-937752c5e79c",
   "metadata": {},
   "source": [
    "While dimensionality reduction techniques offer several benefits in machine learning, they also have some limitations and drawbacks that should be considered. Here are some common limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d0a27-b0a0-4d77-861b-6623a437c2d9",
   "metadata": {},
   "source": [
    "Information Loss: Dimensionality reduction techniques, especially those that aim to discard less informative features, may result in some loss of information. When reducing the dimensionality of the data, there is a trade-off between simplifying the data representation and preserving important characteristics. It is possible that some relevant information or subtle patterns may be lost during the reduction process.\n",
    "\n",
    "Interpretability: Dimensionality reduction can make the interpretation of the transformed data more challenging. As the original features are combined or transformed into new representations, it may become harder to interpret the relationship between the reduced features and the target variable. This can be a drawback if interpretability is a crucial requirement for the task at hand.\n",
    "\n",
    "Algorithm Sensitivity: Different dimensionality reduction techniques may yield different results, and the choice of technique can significantly impact the outcome. The effectiveness of a particular technique may vary depending on the specific dataset and the characteristics of the data. It is important to carefully select an appropriate technique and evaluate its impact on the downstream tasks.\n",
    "\n",
    "Computational Complexity: Some dimensionality reduction techniques can be computationally expensive, particularly when dealing with large datasets or high-dimensional data. Algorithms like t-SNE and certain variants of PCA can require significant computational resources and time to process and transform the data.\n",
    "\n",
    "Curse of Interpretability: While dimensionality reduction can simplify the data representation, it can also introduce a curse of interpretability. The reduced-dimensional space may not have a direct one-to-one correspondence with the original features, making it harder to understand the transformed data in terms of the original features. This can make it more challenging to provide meaningful explanations for the model's predictions or behavior.\n",
    "\n",
    "Overfitting: In some cases, dimensionality reduction techniques can lead to overfitting if not applied carefully. Overfitting can occur when the dimensionality reduction is performed without considering the generalization performance on unseen data. It is important to evaluate the impact of dimensionality reduction on the model's performance using appropriate validation techniques.\n",
    "\n",
    "Sensitivity to Noise: Dimensionality reduction techniques may be sensitive to noisy or irrelevant features in the data. If the dataset contains noisy or irrelevant information, the reduction process may not effectively separate the signal from the noise, leading to suboptimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e066a-9877-4fe2-97b6-8cf491baba8f",
   "metadata": {},
   "source": [
    "Despite these limitations, dimensionality reduction techniques can be valuable tools in preprocessing and analyzing high-dimensional data.\n",
    "\n",
    "It is essential to carefully consider the specific requirements of the task, evaluate the impact of dimensionality reduction on the overall performance, and choose the most appropriate technique based on the characteristics of the data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801f2a1-7fe1-410a-8342-a83c96aa752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ed466-df59-49ef-a641-94f5ac7c7487",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques offer several benefits in machine learning, but they also have limitations and drawbacks that should be considered. Here are some common limitations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a1177-5c67-4856-8036-8cb99bf344ce",
   "metadata": {},
   "source": [
    "1. Information Loss:  Dimensionality reduction techniques can result in the loss of information. When reducing the dimensionality of data, it is inevitable that some information is discarded or compressed. Depending on the technique and the degree of reduction, important features or patterns in the data may be lost, leading to a decrease in the performance of the model.\n",
    "\n",
    "2. Interpretability: As the original features are transformed or combined during dimensionality reduction, the interpretability of the transformed data may be reduced. The reduced-dimensional space may not have a direct one-to-one correspondence with the original features, making it challenging to interpret the relationship between the reduced features and the target variable. This can be a drawback if interpretability is important for understanding the underlying data.\n",
    "\n",
    "3. Curse of Dimensionality: While dimensionality reduction techniques aim to mitigate the curse of dimensionality, they may not completely overcome it. Some techniques may still struggle to capture the most informative features or may be affected by the sparsity of the high-dimensional space. In certain cases, dimensionality reduction may not lead to significant improvements in model performance.\n",
    "\n",
    "4. Algorithmic Complexity and Scalability: Some dimensionality reduction techniques can be computationally expensive, especially for large datasets or high-dimensional data. Algorithms like t-SNE and certain variants of PCA may require significant computational resources and time to process and transform the data. This can limit their scalability in certain scenarios.\n",
    "\n",
    "5. Sensitivity to Parameter Tuning: Dimensionality reduction techniques often have parameters that need to be tuned. The performance of the technique can be sensitive to the choice of parameters, and finding the optimal parameter values can be challenging. Inadequate parameter tuning can lead to suboptimal results or even distort the underlying structure of the data.\n",
    "\n",
    "6. Domain Dependence: The effectiveness of dimensionality reduction techniques can be influenced by the specific domain and the characteristics of the data. Some techniques may work well for certain types of data or tasks but may not be as effective for others. It is important to carefully evaluate the suitability of a technique for a given dataset and problem domain.\n",
    "\n",
    "7. Overfitting: In some cases, dimensionality reduction techniques can lead to overfitting if not applied carefully. Overfitting can occur when the dimensionality reduction is performed without considering the generalization performance on unseen data. It is crucial to evaluate the impact of dimensionality reduction on the model's performance using appropriate validation techniques.\n",
    "\n",
    "8. Preprocessing Challenges: Dimensionality reduction is typically performed as a preprocessing step before applying a machine learning algorithm. This adds an additional step to the workflow and requires careful consideration of data preprocessing and normalization techniques to ensure the effectiveness of dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e6176-2f35-41b6-991f-fcb7bcfd5d9b",
   "metadata": {},
   "source": [
    "Despite these limitations, dimensionality reduction techniques remain valuable tools for handling high-dimensional data and improving machine learning models. \n",
    "\n",
    "It is important to carefully consider the specific requirements of the task, evaluate the impact of dimensionality reduction on the overall performance, and choose the most appropriate technique based on the characteristics of the data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb9415-63f1-4e58-98f3-98f3a362ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b2aae-5054-49ac-a65f-b9dc87441acf",
   "metadata": {},
   "source": [
    "The curse of dimensionality is closely related to overfitting and underfitting in machine learning. Here's how they are connected:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2c81d-bce4-4561-9b44-01d22839c7b0",
   "metadata": {},
   "source": [
    "1. Curse of Dimensionality and Overfitting: The curse of dimensionality refers to the challenges and issues that arise when dealing with high-dimensional data. As the number of features or dimensions increases, the data becomes more spread out, and the available data points become sparser. In high-dimensional spaces, it becomes easier for models to fit noise or spurious patterns in the training data, leading to overfitting. Overfitting occurs when a model learns the specific details and noise of the training data too well, but fails to generalize to unseen data. The increased dimensionality exacerbates the risk of overfitting as the model has more flexibility to capture complex relationships within the training data, even if they are not representative of the underlying true patterns.\n",
    "\n",
    "2. Curse of Dimensionality and Underfitting: On the other hand, the curse of dimensionality can also contribute to underfitting in certain cases. Underfitting occurs when a model is too simple and fails to capture the underlying patterns in the data. In high-dimensional spaces, where the available data is sparser, it becomes more challenging for a simple model to capture the complex relationships and patterns present in the data. As a result, an underfit model may struggle to find meaningful patterns and may generalize poorly to unseen data.\n",
    "\n",
    "3. Addressing the Curse of Dimensionality: To address the curse of dimensionality and mitigate the risks of overfitting and underfitting, various techniques are employed. Dimensionality reduction techniques, such as PCA or feature selection, can help reduce the number of dimensions and capture the most informative features. By reducing the dimensionality, these techniques can simplify the data representation, remove noise or irrelevant features, and improve the generalization ability of the model. Regularization techniques, such as L1 or L2 regularization, can also help prevent overfitting by imposing constraints on the model's complexity and reducing the impact of irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa48e9c-1a41-40d7-a6fc-12f2d53d3863",
   "metadata": {},
   "source": [
    "In summary, the curse of dimensionality impacts machine learning models by increasing the risk of overfitting due to the increased flexibility and sparsity of high-dimensional data. It also poses challenges for underfitting, as simple models may struggle to capture complex relationships.\n",
    "\n",
    "Techniques such as dimensionality reduction and regularization play a vital role in addressing the curse of dimensionality and striking a balance between overfitting and underfitting, enabling models to effectively learn and generalize from high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8b1c2-2cda-4035-aaba-84924e5be985",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6425224-fe7b-451b-8854-da6c0917aa4f",
   "metadata": {},
   "source": [
    "Determining the optimal number of dimensions to reduce data to in dimensionality reduction techniques can be approached in several ways. Here are a few common methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67de30-583d-4919-87f2-694ac15858d1",
   "metadata": {},
   "source": [
    "1. Variance Retention: For techniques like Principal Component Analysis (PCA), which aim to capture the maximum variance in the data, the cumulative explained variance can be used to determine the optimal number of dimensions. The explained variance is calculated for each principal component, and the cumulative explained variance is plotted against the number of dimensions. The number of dimensions where the cumulative variance reaches a desired threshold (e.g., 90% or 95%) can be chosen as the optimal number of dimensions.\n",
    "\n",
    "2. Scree Plot: In PCA, the scree plot can provide insights into the optimal number of dimensions. The scree plot visualizes the eigenvalues or explained variances of the principal components in decreasing order. The plot shows the proportion of variance explained by each component. The optimal number of dimensions can be determined by identifying the point where the eigenvalues or explained variances drop off significantly, indicating that additional dimensions provide diminishing returns in terms of explained variance.\n",
    "\n",
    "3. Cross-Validation: Cross-validation can be employed to estimate the performance of the dimensionality reduction technique at different numbers of dimensions. The data is transformed using different numbers of dimensions, and the model's performance (e.g., accuracy, error rate) is evaluated using cross-validation. The number of dimensions that yields the best performance on the validation set can be selected as the optimal number.\n",
    "\n",
    "4. Information Criteria: Information criteria, such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), can be used to determine the optimal number of dimensions. These criteria balance the model's goodness of fit with its complexity. Different numbers of dimensions are evaluated, and the information criteria are calculated. The number of dimensions that minimizes the information criteria can be chosen as the optimal number.\n",
    "\n",
    "5. Domain Knowledge and Interpretability: The optimal number of dimensions can also be influenced by domain knowledge and interpretability requirements. Consider the specific problem and task at hand. Evaluate the trade-off between the reduced dimensionality and the interpretability or utility of the transformed data. In some cases, the optimal number of dimensions may be based on the desired interpretability or the specific requirements of downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d637e-3c85-44aa-b810-309cd48afcc7",
   "metadata": {},
   "source": [
    "It's important to note that the optimal number of dimensions may vary depending on the dataset, the dimensionality reduction technique used, and the specific goals of the analysis. \n",
    "\n",
    "It's recommended to explore multiple methods and evaluate the impact of different numbers of dimensions on the overall performance of the downstream tasks to make an informed decision about the optimal number of dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783a007-de42-4f8d-a086-cd687a9e4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f234237-3966-4637-837c-ab54fd2d6c83",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It states that the probability of a hypothesis (H) given some observed evidence (E) is proportional to the probability of the evidence given the hypothesis and the prior probability of the hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cfed2f-510c-4953-8f9e-4c125b8bfb15",
   "metadata": {},
   "source": [
    "Mathematically, Bayes' theorem can be expressed as:\n",
    "\n",
    "P(H | E) = P(E | H) * P(H) / P(E)\n",
    "\n",
    "where:\n",
    "\n",
    "P(H | E) is the posterior probability of the hypothesis given the evidence, which is what we want to calculate.\n",
    "\n",
    "P(E | H) is the likelihood of the evidence given the hypothesis, which measures how well the hypothesis explains the evidence.\n",
    "\n",
    "P(H) is the prior probability of the hypothesis, which reflects our belief about the hypothesis before we observe any evidence.\n",
    "\n",
    "P(E) is the probability of the evidence, which serves as a normalization constant to ensure that the posterior probability is a valid probability distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cab289-78c2-4203-888b-f0e7f8d019c1",
   "metadata": {},
   "source": [
    "\n",
    "Bayes' theorem is widely used in many fields, including statistics, machine learning, and artificial intelligence. It provides a principled way to update our beliefs in light of new evidence and to make predictions or decisions based on uncertain information. \n",
    "\n",
    "In Bayesian inference, we start with a prior distribution that represents our initial belief about the unknown quantity of interest, and then we update the distribution using Bayes' theorem to obtain a posterior distribution that incorporates the observed data. The posterior distribution summarizes our updated knowledge about the unknown quantity and can be used for inference or decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3943c-4f94-4f89-87d6-182620330082",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc9779-cf1b-4587-8ec5-0de71cbae9aa",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis given new evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef8a3a-a633-4ef9-8d78-ca720ad87c43",
   "metadata": {},
   "source": [
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the probability of hypothesis A given the evidence B (also known as the posterior probability).\n",
    "P(B|A) is the probability of the evidence B given hypothesis A (also known as the likelihood).\n",
    "P(A) is the prior probability of hypothesis A (i.e., the probability of A before taking into account the evidence B).\n",
    "P(B) is the probability of the evidence B (i.e., the probability of observing the evidence B regardless of the hypothesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f6f9c-0262-4669-ad0a-d9dc0ca6f47c",
   "metadata": {},
   "source": [
    "The formula states that the posterior probability of a hypothesis A given the evidence B is proportional to the likelihood of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis. This quantity is then divided by the probability of the evidence B, which acts as a normalization factor to ensure that the posterior probability is a valid probability distribution."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed917d75-3e74-4f0a-8891-a4a397daf9fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa3982-be65-47ac-9ef2-65ded1311e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e815887-6033-4123-bf5a-3789b7e07579",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in a wide range of fields, including statistics, machine learning, natural language processing, and artificial intelligence. It is a powerful tool for reasoning under uncertainty and updating beliefs based on new evidence. Some practical applications of Bayes' theorem include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4eb494-5cb9-49ba-8840-e8b2d56c1910",
   "metadata": {},
   "source": [
    "Medical diagnosis: Bayes' theorem is often used in medical diagnosis to calculate the probability of a disease given a set of symptoms. For example, given a patient's symptoms, a doctor can use Bayes' theorem to calculate the probability of the patient having a certain disease.\n",
    "\n",
    "Spam filtering: Bayes' theorem is used in spam filtering to calculate the probability of an email being spam given its content. This is done by training a machine learning model on a dataset of emails that are labeled as either spam or not spam. The model then uses Bayes' theorem to calculate the probability that a new email is spam given its content.\n",
    "\n",
    "Natural language processing: Bayes' theorem is used in natural language processing to perform tasks such as sentiment analysis and text classification. For example, given a piece of text, Bayes' theorem can be used to calculate the probability that it belongs to a certain category (such as positive or negative sentiment).\n",
    "\n",
    "Financial modeling: Bayes' theorem is used in financial modeling to estimate the probability of certain events (such as a stock market crash) given historical data. This can help investors make better decisions about their investments.\n",
    "\n",
    "Overall, Bayes' theorem is a versatile and powerful tool that can be used in many different fields to reason under uncertainty and make better decisions based on new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5b546-f85c-4c3f-8a2f-54369306f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b782d-71b4-44d5-8ec1-b9e4f62dadb5",
   "metadata": {},
   "source": [
    "Bayes' theorem is a formula that relates conditional probabilities. In fact, Bayes' theorem is simply a way of expressing conditional probabilities in a different form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2fd4b-e319-4d8e-ad1f-4920f6175bee",
   "metadata": {},
   "source": [
    "Conditional probability is the probability of an event A given that another event B has occurred, and it is written as P(A|B). Bayes' theorem relates this to the probability of B given A, which is written as P(B|A)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dd569-42b6-4a8b-9401-ce0f6991f96e",
   "metadata": {},
   "source": [
    "Specifically, Bayes' theorem states that the probability of A given B is equal to the probability of B given A, multiplied by the prior probability of A, and then divided by the marginal probability of B. This can be written as:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e2a22-4a4d-4bdd-99e6-1f41b0b06f5b",
   "metadata": {},
   "source": [
    "In other words, Bayes' theorem provides a way of updating our beliefs about the probability of an event A given new evidence B. It tells us how much we should revise our belief in A based on the likelihood of B given A, as well as our prior belief in A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6647c-9820-424a-8632-ee06fef844ca",
   "metadata": {},
   "source": [
    "Conditional probability is a key concept in probability theory, and Bayes' theorem is a powerful tool that allows us to reason about conditional probabilities and update our beliefs based on new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d71dd8-b679-4d59-aae9-4012dd412e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113ea04-c18e-47e9-afd5-70b2d03d509e",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of simple but powerful algorithms that are widely used in machine learning for classification problems. There are three main types of Naive Bayes classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989ee3f-875d-45db-bbe8-ec0c0440a5dc",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes: This classifier is used for continuous input variables that are assumed to be normally distributed. It is suitable for problems where the input features are real-valued and have a Gaussian distribution, such as in many image and speech recognition tasks.\n",
    "\n",
    "Multinomial Naive Bayes: This classifier is used for discrete input variables that represent counts or frequencies. It is commonly used in text classification problems, where the input features represent word frequencies or bag-of-words representations.\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier is similar to the multinomial Naive Bayes, but it assumes that the input variables are binary (i.e., take on values of 0 or 1). It is commonly used in text classification problems where the input features represent the presence or absence of words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d5a41-3be2-4f7c-8543-f9a36cc01127",
   "metadata": {},
   "source": [
    "The choice of which type of Naive Bayes classifier to use for a given problem depends on the nature of the input features and the problem domain. In general, Gaussian Naive Bayes is best suited for continuous-valued features, while multinomial and Bernoulli Naive Bayes are more appropriate for discrete-valued features, such as those commonly encountered in text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61abcd4-80b2-4888-a0d1-d22c5ed2aa97",
   "metadata": {},
   "source": [
    "It is important to note that Naive Bayes classifiers make a strong assumption of independence between the input features, which may not be true in many real-world problems. Therefore, it is often a good idea to try different types of classifiers, including non-Naive Bayes classifiers, and compare their performance on the given problem before making a final choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91544fc1-b0ae-46ae-a919-1edc0d595202",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 6:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed01d7-4e7e-466c-a77a-e82782455a5c",
   "metadata": {},
   "source": [
    "To use Naive Bayes to classify the new instance with features X1=3 and X2=4, we need to calculate the probability of each class given these feature values. We can do this using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae168dc-8270-452c-8f17-d3fee7382c2b",
   "metadata": {},
   "source": [
    "Assuming equal prior probabilities for each class, i.e. P(A) = P(B) = 0.5, we can simplify the above equations to:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) / P(X1=3,X2=4)\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) / P(X1=3,X2=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd5fa6-a8b2-42bd-a072-afe954f95bfc",
   "metadata": {},
   "source": [
    "To calculate the probabilities, we first need to estimate the conditional probabilities P(X1,X2|A) and P(X1,X2|B) from the given dataset. Since the features are assumed to be conditionally independent given the class, we can calculate these probabilities as:\n",
    "\n",
    "P(X1,X2|A) = P(X1|A) * P(X2|A)\n",
    "P(X1,X2|B) = P(X1|B) * P(X2|B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846842a-5dfd-4b7c-a1e1-0a0d68d7d72a",
   "metadata": {},
   "source": [
    "Using the frequency table given in the problem, we can calculate the conditional probabilities as follows:\n",
    "\n",
    "P(X1=3|A) = 4/10\n",
    "P(X1=3|B) = 1/10\n",
    "\n",
    "P(X2=4|A) = 3/10\n",
    "P(X2=4|B) = 1/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebeaa2-fd41-4bab-bf79-df61f753f9b3",
   "metadata": {},
   "source": [
    "Therefore:\n",
    "\n",
    "P(X1,X2|A) = P(X1=3|A) * P(X2=4|A) = (4/10) * (3/10) = 12/100\n",
    "P(X1,X2|B) = P(X1=3|B) * P(X2=4|B) = (1/10) * (1/5) = 1/50\n",
    "\n",
    "Next, we need to calculate the marginal probability of the features P(X1=3,X2=4):\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)\n",
    "= (12/100) * 0.5 + (1/50) * 0.5\n",
    "= 0.055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c894a7-15a6-47fb-b618-a24edd693f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we can calculate the posterior probabilities:\n",
    "\n",
    "P(A|X1=3,X2=4) = (12/100) * 0.5 / 0.055 = 0.545\n",
    "P(B|X1=3,X2=4) = (1/50) * 0.5 / 0.055 = 0.455\n",
    "\n",
    "Therefore, the Naive Bayes classifier would predict that the new instance belongs to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

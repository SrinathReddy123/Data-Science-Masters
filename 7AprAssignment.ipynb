{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ecc15-b1e3-46e0-9df6-10b9b655bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78f406-dc3e-4724-aa06-3e4b15fa6435",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both types of functions that are commonly used in machine learning algorithms for feature mapping, which is the process of transforming the input features into a higher-dimensional space to make them more separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dc05d-f43e-4a40-a276-76bfa69d103a",
   "metadata": {},
   "source": [
    "Polynomial functions are a type of explicit feature mapping, which means that the transformed features are defined explicitly in terms of the input features. In other words, a polynomial function takes the input features x and maps them to a new feature space by applying a polynomial function of degree d:\n",
    "\n",
    "Φ(x) = [1, x, x², x³, ..., xd]\n",
    "\n",
    "This transforms each input feature into a polynomial of degree d, and the resulting feature space has (d+1) dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab22d6e-089d-41a3-abf5-9351f76decc4",
   "metadata": {},
   "source": [
    "Kernel functions, on the other hand, are a type of implicit feature mapping, which means that the transformed features are not explicitly defined. Instead, the kernel function computes the inner product between the transformed features without actually computing the transformation itself. The kernel function represents a similarity measure between pairs of instances in the original input space and is defined as:\n",
    "\n",
    "K(x, y) = Φ(x)TΦ(y)\n",
    "\n",
    "where Φ is an implicit feature mapping. The advantage of using kernel functions is that they allow us to work in a higher-dimensional feature space without actually computing the explicit feature mapping, which can be computationally expensive or even impossible in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8017af-06cc-47c1-8364-8644a986c650",
   "metadata": {},
   "source": [
    "In summary, both polynomial functions and kernel functions are used for feature mapping in machine learning algorithms, but polynomial functions are a type of explicit feature mapping that transforms the input features into a higher-dimensional space by applying a polynomial function, while kernel functions are a type of implicit feature mapping that computes the similarity between pairs of instances in the original input space without actually computing the explicit feature mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c22682-cb5f-4131-b391-c389104ec322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb92d8-7aa3-4550-8446-b60d70fecf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5970366-c5e7-4b86-91e0-b480a5bad354",
   "metadata": {},
   "outputs": [],
   "source": [
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, we can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8abd62-9ede-4623-bf33-8e0de5a01b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries:\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603e52b6-ff10-4b56-8b2e-85cb6ab2dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset:\n",
    "    \n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11a1319-115a-4637-a12f-142350d1ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16d6e01-9264-4f4d-8267-cbf8991a9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM classifier object with a polynomial kernel:\n",
    "\n",
    "svm_classifier = svm.SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Here, we are using a third-degree polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0b269c-fa18-41c5-91f6-92a1ef26ff70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we are using a third-degree polynomial kernel.\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666eae2a-b69e-40f5-a422-b436c88f37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the classifier on the testing data:\n",
    "\n",
    "accuracy = svm_classifier.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d88ea5-f9fd-4f82-9898-603ae874bf00",
   "metadata": {},
   "source": [
    "This will give us the classification accuracy of the model on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03636380-1250-419b-b03a-0b7047c376ce",
   "metadata": {},
   "source": [
    "We can also tune the hyperparameters of the SVM classifier, such as the degree of the polynomial kernel, the regularization parameter C, and the coefficient of the kernel function gamma, using cross-validation and grid search. Here is an example of how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ee79f-19ba-4a8c-9f2a-aa4065a7b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'degree': [2, 3, 4], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and corresponding accuracy score\n",
    "best_params = grid_search.best_params_\n",
    "accuracy = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afabef87-167c-4d51-889a-20bd533788c5",
   "metadata": {},
   "source": [
    "Here, we are using a grid search with cross-validation to find the best hyperparameters for the SVM classifier. We can then use the best hyperparameters to train the model and evaluate its performance on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06d07f-4218-4973-9225-c9f21e7dc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a68d4-8c7a-4fff-89cb-340403a4a1b5",
   "metadata": {},
   "source": [
    "In support vector regression (SVR), epsilon is a hyperparameter that controls the width of the margin around the predicted function within which no penalty is incurred. \n",
    "\n",
    "It represents the maximum deviation between the predicted value and the true value that is allowed before the corresponding data point is considered a violation and incurs a penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bce766-e458-4acd-b4ce-7301f05b2452",
   "metadata": {},
   "source": [
    "Increasing the value of epsilon will increase the number of support vectors in SVR. This is because a larger epsilon value will allow more data points to be within the margin, resulting in a larger area around the predicted function that is not penalized.\n",
    "\n",
    "As a result, more data points will be considered as support vectors as they are now inside the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd94514-ef7f-488f-96a3-0565e9c5b9c4",
   "metadata": {},
   "source": [
    "However, it is important to note that increasing the number of support vectors may lead to longer training times and slower predictions, as the model needs to consider more data points during both training and testing.\n",
    "\n",
    "Therefore, it is important to carefully choose the value of epsilon to balance between the number of support vectors and the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b489fe0-0ec9-4ff8-89a8-70b428961c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8773b5f-bb5c-4841-aec6-2eb995bcea77",
   "metadata": {},
   "source": [
    "The performance of Support Vector Regression (SVR) is affected by several hyperparameters, including the choice of kernel function, C parameter, epsilon parameter, and gamma parameter.\n",
    "\n",
    "Each of these parameters works differently and can be tuned to improve the performance of the model. Below is a detailed explanation of each parameter and how it affects the performance of SVR:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24861ca-bbc7-40ba-8cd0-5e5d7f717e82",
   "metadata": {},
   "source": [
    "Kernel function: \n",
    "\n",
    "The kernel function is used to transform the input data into a higher-dimensional space to enable non-linear separation of the data. There are several types of kernel functions available in scikit-learn, including linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "\n",
    "The choice of kernel function depends on the nature of the data and the problem being solved. For example, if the data has a clear linear separation, a linear kernel function may be sufficient. On the other hand, if the data is non-linearly separable, a polynomial or RBF kernel function may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883fd49-ee38-45fb-9413-ee1b6ad6acb4",
   "metadata": {},
   "source": [
    "C parameter: \n",
    "\n",
    "The C parameter controls the trade-off between achieving a low training error and a low testing error. A small C value allows for more errors in the training set, which may result in a simpler model with a wider margin, but it may also result in higher errors on the testing set. \n",
    "\n",
    "Conversely, a large C value will result in a smaller margin and a more complex model that may overfit the training set but may also result in better performance on the testing set. Increasing C will make the model more complex and more prone to overfitting, while decreasing C will make the model simpler and less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe76ef-c828-483c-93f0-e058d4ec8bee",
   "metadata": {},
   "source": [
    "Epsilon parameter:\n",
    "\n",
    "The epsilon parameter controls the width of the margin around the predicted function within which no penalty is incurred. It represents the maximum deviation between the predicted value and the true value that is allowed before the corresponding data point is considered a violation and incurs a penalty. \n",
    "\n",
    "Increasing the value of epsilon will allow more data points to be within the margin and may result in a larger number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0ecc1-c6c5-44d6-a1df-c8ed8550abc2",
   "metadata": {},
   "source": [
    "Gamma parameter: \n",
    "The gamma parameter controls the smoothness of the decision boundary. A small gamma value results in a smoother decision boundary and may result in underfitting, while a large gamma value results in a more complex decision boundary that may overfit the data. \n",
    "\n",
    "Increasing gamma will result in a more complex model, while decreasing gamma will result in a simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1196c0-7ada-4a5b-b703-cd929494d57d",
   "metadata": {},
   "source": [
    "In summary, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affects the performance of SVR in different ways. Each parameter can be tuned to improve the performance of the model, but the optimal values may depend on the nature of the data and the problem being solved.\n",
    "\n",
    "A good strategy is to try different combinations of hyperparameters using cross-validation and choose the combination that gives the best performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9551a-b716-4176-baf2-ec91671359bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6c606-55a5-4eb5-97c9-c6c60427df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57748a3f-3b34-4e6f-80d6-94cb15c876af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd35af-4c7b-4ddc-bf2a-964e0dd9333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8f8e3-2ab5-4de1-87f1-bee6164294e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d379bf-1815-420e-9015-632e413b1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78795f9-35a1-4143-86fe-4ad6796aa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150768a7-bffa-4fcd-a620-089b23ff1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57278d1-e8c4-406f-b62e-3f4577f38b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6594145-ae2a-4145-a78a-06e58d676967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1f2c6-ba88-4753-8f3b-757d8418a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained classifier to predict the labels of the testing data\n",
    "\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921d6cd-0ef1-4994-8a1f-0f7723a4cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the classifier using accuracy score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5de6c-4dfd-47d9-8596-6d0a784e636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844139f-e2bc-4fb6-8429-b732c699d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tuned classifier on the entire dataset\n",
    "\n",
    "tuned_svc = grid_search.best_estimator_\n",
    "tuned_svc.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094d238-7879-41d4-8c61-b7d1653c7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier to a file for future use\n",
    "\n",
    "with open('tuned_svc.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_svc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbe3a2-6e46-461c-91da-737881f8c519",
   "metadata": {},
   "source": [
    "In this example, we first load the Iris dataset and split it into training and testing sets. We then preprocess the data using StandardScaler to scale the features to have zero mean and unit variance.\n",
    "\n",
    "We create an instance of the SVC classifier and fit it on the training data. We then use the trained classifier to predict the labels of the testing data and evaluate its performance using accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d5947-92b9-49fc-8796-cc4e796fa946",
   "metadata": {},
   "source": [
    "Next, we tune the hyperparameters of the SVC classifier using GridSearchCV with a range of values for the C and gamma parameters and two kernel functions: linear and radial basis function (RBF).\n",
    "\n",
    "We set cv=5 for 5-fold cross-validation. We then train the tuned classifier on the entire dataset and save it to a file for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
